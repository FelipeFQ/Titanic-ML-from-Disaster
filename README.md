# ğŸš¢ Titanic - Machine Learning from Disaster

A complete end-to-end **data science project** solving the classic Kaggle Titanic survival prediction challenge.  
This repository demonstrates **data exploration, feature engineering, and machine learning modeling**, wrapped in a reproducible environment for transparency and reusability.

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ train.csv              # Training dataset
â”‚   â”œâ”€â”€ test.csv               # Test dataset
â”‚   â”œâ”€â”€ gender_submission.csv  # Sample submission file
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ data_overview.ipynb    # Dataset understanding and inspection
â”‚   â”œâ”€â”€ eda.ipynb              # Exploratory data analysis and feature insights
â”‚   â”œâ”€â”€ modeling.ipynb         # Model building, evaluation, and predictions
â”‚
â”œâ”€â”€ outputs/                   #  Saved models, submission files
â”‚
â”œâ”€â”€ .gitignore                # Git ignore rules
â”œâ”€â”€ requirements.txt          # Python package dependencies
â”œâ”€â”€ titanic_environment.yml   # Conda environment for reproducibility
â””â”€â”€ README.md                 # Project documentation
```

---

## ğŸ¯ Objective

The goal of this project is to predict **which passengers survived** the Titanic shipwreck based on features such as age, sex, socio-economic class, and family relations.  

This problem is a great introduction to:

- Data cleaning and preprocessing  
- Feature engineering and selection  
- Model comparison and evaluation  
- Reproducible ML workflows  

---

## ğŸ” Workflow

### 1. Data Overview (`data_overview.ipynb`)
- Import and inspect datasets (`train.csv`, `test.csv`, `gender_submission.csv`).  
- Check missing values, data types, and basic distributions.  

### 2. Exploratory Data Analysis (`eda.ipynb`)
- Visual exploration of key survival patterns.  
- Feature relationships with target variable (`Survived`).  
- Handling missing values (e.g., `Age`, `Cabin`, `Embarked`).  
- Transformations and encoding (categorical â†’ numeric).  

### 3. Modeling (`modeling.ipynb`)
- Train/test split and preprocessing pipelines.  
- Baseline models (Logistic Regression, Decision Trees).  
- Advanced models (Random Forest, Gradient Boosting, XGBoost, LightGBM).  
- Hyperparameter tuning and cross-validation.  
- Feature importance visualization.  
- Submission file creation for Kaggle.  

---

## âš™ï¸ Environment & Dependencies

All dependencies are captured in [`titanic_environment.yml`](./titanic_environment.yml).  

### Key Libraries
- **Data analysis**: `pandas`, `numpy`  
- **Visualization**: `matplotlib`, `seaborn`  
- **Machine Learning**: `scikit-learn`, `xgboost`, `lightgbm`, `shap`  
- **Notebooks**: `jupyterlab`, `ipykernel`

### ğŸ”§ Setup

Create the conda environment:

```bash
conda env create -f titanic_environment.yml
conda activate titanic
```

Or install requirements manually (example):

```bash
pip install pandas numpy scikit-learn matplotlib seaborn xgboost lightgbm shap
```

---

## ğŸ“Š Results

- Models achieve competitive accuracy scores (~0.77) on validation.  
- Ensemble approaches (LightGBM/XGBoost) outperform simpler baselines.  
- Feature importance shows **Sex, Pclass, Age, Fare** as key survival indicators.  
- Final predictions are exported in Kaggle-compatible format (`submission.csv`).  

---

## âœ¨ Skills Demonstrated

- **Data Wrangling**: Handling missing data, feature engineering.  
- **Visualization**: Identifying insights with plots and distributions.  
- **ML Modeling**: Building, tuning, and comparing models.  
- **Reproducibility**: Using `conda` environments and structured notebooks.  
- **Communication**: Documenting workflow and presenting results clearly.  

---

## ğŸ“Œ Next Steps

- Try deep learning approaches (e.g., neural networks).  
- Explore ensemble stacking for improved performance.  
- Automate pipeline with `scikit-learn`â€™s `Pipeline` or `mlflow`.  

---

## ğŸ† Kaggle Competition Link
ğŸ‘‰ [Titanic - Machine Learning from Disaster](https://www.kaggle.com/c/titanic)
